{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073890d2-45be-4b74-829e-beb38ad373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f7935-6690-46d9-9e33-111d62011a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_lists, gold_lists):\n",
    "    hits_at_10_count = 0\n",
    "    hits_at_4_count = 0\n",
    "    map_at_10_list = []\n",
    "    mrr_list = []\n",
    "\n",
    "    for retrieved, gold in zip(retrieved_lists, gold_lists):\n",
    "        hits_at_10_flag = False\n",
    "        hits_at_4_flag = False\n",
    "        average_precision_sum = 0\n",
    "        first_relevant_rank = None\n",
    "        find_gold = []\n",
    "\n",
    "        gold = [item.replace(\" \", \"\").replace(\"\\n\", \"\") for item in gold]\n",
    "        retrieved = [item.replace(\" \", \"\").replace(\"\\n\", \"\") for item in retrieved]\n",
    "\n",
    "        for rank, retrieved_item in enumerate(retrieved[:11], start=1):\n",
    "            if any(gold_item in retrieved_item for gold_item in gold):\n",
    "                if rank <= 10:\n",
    "                    hits_at_10_flag = True\n",
    "                    if first_relevant_rank is None:\n",
    "                        first_relevant_rank = rank\n",
    "                    if rank <= 4:\n",
    "                        hits_at_4_flag = True\n",
    "                    # Compute precision at this rank for this query\n",
    "                    count = 0\n",
    "                    for gold_item in gold:\n",
    "                        if gold_item in retrieved_item and not gold_item in find_gold:\n",
    "                            count =  count + 1\n",
    "                            find_gold.append(gold_item)\n",
    "                    precision_at_rank = count / rank\n",
    "                    average_precision_sum += precision_at_rank\n",
    "\n",
    "        # Calculate metrics for this query\n",
    "        hits_at_10_count += int(hits_at_10_flag)\n",
    "        hits_at_4_count += int(hits_at_4_flag)\n",
    "        map_at_10_list.append(average_precision_sum / min(len(gold), 10))\n",
    "        mrr_list.append(1 / first_relevant_rank if first_relevant_rank else 0)\n",
    "\n",
    "    # Calculate average metrics over all queries\n",
    "    hits_at_10 = hits_at_10_count / len(gold_lists)\n",
    "    hits_at_4 = hits_at_4_count / len(gold_lists)\n",
    "    map_at_10 = sum(map_at_10_list) / len(gold_lists)\n",
    "    mrr_at_10 = sum(mrr_list) / len(gold_lists)\n",
    "\n",
    "    return {\n",
    "        'Hits@10': hits_at_10,\n",
    "        'Hits@4': hits_at_4,\n",
    "        'MAP@10': map_at_10,\n",
    "        'MRR@10': mrr_at_10,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce998961-5188-4aac-af72-adef8fe0e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_eval(file_name):\n",
    "    print(f'For file: {file_name}')\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    retrieved_lists = []\n",
    "    gold_lists  = []\n",
    "\n",
    "    for d in data:\n",
    "        if d['question_type'] == 'null_query':\n",
    "            continue\n",
    "        retrieved_lists.append([m['text'] for m in d['retrieval_list']])\n",
    "        gold_lists.append([m['fact'] for m in d['gold_list']])\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(retrieved_lists, gold_lists)\n",
    "\n",
    "    # Print the metrics\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af7fed-cc7c-46a3-9593-1ea5ea748ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "#stage_one_filename = 'output/llm-embedder-ranker.json'\n",
    "stage_one_filename = 'output/bge-reranker-large.json'\n",
    "main_eval(stage_one_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
