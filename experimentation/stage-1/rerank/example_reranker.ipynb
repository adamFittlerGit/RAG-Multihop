{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d8f44-6485-474f-9802-5afc6fedd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from typing import Any, Generator, List, Dict, Optional\n",
    "\n",
    "import openai\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    OpenAIEmbedding,\n",
    "    PromptHelper,\n",
    "    VectorStoreIndex,\n",
    "    set_global_service_context\n",
    ")\n",
    "from llama_index.extractors import BaseExtractor\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.cohereai import CohereEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.embeddings import HuggingFaceEmbedding,VoyageEmbedding,InstructorEmbedding\n",
    "from llama_index.postprocessor import FlagEmbeddingReranker\n",
    "from llama_index.schema import QueryBundle,MetadataMode\n",
    "from llama_index.schema import Document\n",
    "import asyncio\n",
    "#import torch\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "# This is the staging flag. Set to False if you want to run on the real\n",
    "# collection.\n",
    "# STAGING=False\n",
    "STAGING=True\n",
    "\n"
   ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab27ef-50ae-44d2-88ab-81cbb5dd52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_json(lst, filename):\n",
    "  \"\"\" Save Files \"\"\"\n",
    "  with open(filename, 'w') as file:\n",
    "    json.dump(lst, file)\n",
    "\n",
    "def wr_dict(filename,dic):\n",
    "  \"\"\" Write Files \"\"\"\n",
    "  try:\n",
    "    if not os.path.isfile(filename):\n",
    "      data = []\n",
    "      data.append(dic)\n",
    "      with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    else:\n",
    "      with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        data.append(dic)\n",
    "      with open(filename, 'w') as f:\n",
    "          json.dump(data, f)\n",
    "  except Exception as e:\n",
    "    print(\"Save Error:\", str(e))\n",
    "  return\n",
    "\n",
    "def rm_file(file_path):\n",
    "  \"\"\" Delete Files \"\"\"\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"File {file_path} removed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91115997-0b02-4951-a913-53075d03d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depth_first_yield(json_data: Any, levels_back: int, collapse_length:\n",
    "                       Optional[int], path: List[str], ensure_ascii: bool = False,\n",
    "                      ) -> Generator[str, None, None]:\n",
    "  \"\"\" Do depth first yield of all of the leaf nodes of a JSON.\n",
    "      Combines keys in the JSON tree using spaces.\n",
    "      If levels_back is set to 0, prints all levels.\n",
    "      If collapse_length is not None and the json_data is <= that number\n",
    "      of characters, then we collapse it into one line.\n",
    "  \"\"\"\n",
    "  if isinstance(json_data, (dict, list)):\n",
    "    # only try to collapse if we're not at a leaf node\n",
    "    json_str = json.dumps(json_data, ensure_ascii=ensure_ascii)\n",
    "    if collapse_length is not None and len(json_str) <= collapse_length:\n",
    "      new_path = path[-levels_back:]\n",
    "      new_path.append(json_str)\n",
    "      yield \" \".join(new_path)\n",
    "      return\n",
    "    elif isinstance(json_data, dict):\n",
    "      for key, value in json_data.items():\n",
    "        new_path = path[:]\n",
    "        new_path.append(key)\n",
    "        yield from _depth_first_yield(value, levels_back, collapse_length, new_path)\n",
    "    elif isinstance(json_data, list):\n",
    "      for _, value in enumerate(json_data):\n",
    "        yield from _depth_first_yield(value, levels_back, collapse_length, path)\n",
    "    else:\n",
    "      new_path = path[-levels_back:]\n",
    "      new_path.append(str(json_data))\n",
    "      yield \" \".join(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49d8cf-07ad-4157-b1e8-56cdc4afda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two classes are used to parse the json corpus and queries.\n",
    "class JSONReader():\n",
    "  \"\"\"JSON reader.\n",
    "     Reads JSON documents with options to help suss out relationships between nodes.\n",
    "  \"\"\"\n",
    "  def __init__(self, is_jsonl: Optional[bool] = False,) -> None:\n",
    "    \"\"\"Initialize with arguments.\"\"\"\n",
    "    super().__init__()\n",
    "    self.is_jsonl = is_jsonl\n",
    "\n",
    "  def load_data(self, input_file: str) -> List[Document]:\n",
    "    \"\"\"Load data from the input file.\"\"\"\n",
    "    documents = []\n",
    "    with open(input_file, 'r') as file:\n",
    "      load_data = json.load(file)\n",
    "    for data in load_data:\n",
    "      metadata = {\"title\": data['title'],\n",
    "                  \"published_at\": data['published_at'],\n",
    "                  \"source\":data['source']}\n",
    "      documents.append(Document(text=data['body'], metadata=metadata))\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf240f-94a8-4990-b45c-3bcd72209a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomExtractor(BaseExtractor):\n",
    "    async def aextract(self, nodes) -> List[Dict]:\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"title\": (\n",
    "                    node.metadata[\"title\"]\n",
    "                ),\n",
    "                \"source\": (\n",
    "                    node.metadata[\"source\"]\n",
    "                ),\n",
    "                \"published_at\": (\n",
    "                    node.metadata[\"published_at\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b28f2-db0c-4c6f-b58f-37649fc2437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gen_stage_0(corpus, queries, rank_model_name, rerank, rerank_model_name, output_name):\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API_KEY\", \"your_openai_api_key\")\n",
    "    openai.base_url = \"your_api_base\"\n",
    "    voyage_api_key = os.environ.get(\"VOYAGE_API_KEY\", \"your_voyage_api_key\")\n",
    "    cohere_api_key = os.environ.get(\"COHERE_API_KEY\", \"your_cohere_api_key\")\n",
    "    model_name = rank_model_name\n",
    "    def_llm = \"gpt-3.5-turbo-1106\"\n",
    "    topk = 10\n",
    "    chunk_size = 256\n",
    "    context_window = 2048\n",
    "    num_output = 256\n",
    "    save_file = output_name\n",
    "    model_name = rank_model_name\n",
    "    llm = OpenAI(model=def_llm, temperature=0, max_tokens=context_window)\n",
    "\n",
    "\n",
    "    print('Remove save file if exists.')\n",
    "    rm_file(save_file)\n",
    "\n",
    "    # Most likely you only need the HuggingFaceEmbedding, but I try to account\n",
    "    # for many other possibilities.\n",
    "    if 'text' in model_name:\n",
    "        # \"text-embedding-ada-002\" “text-search-ada-query-001”\n",
    "        embed_model = OpenAIEmbedding(model = model_name,embed_batch_size=10)\n",
    "    elif 'Cohere' in model_name:\n",
    "        embed_model = CohereEmbedding(\n",
    "            cohere_api_key=cohere_api_key,\n",
    "            model_name=\"embed-english-v3.0\",\n",
    "            input_type=\"search_query\",\n",
    "        )\n",
    "    elif 'voyage-02' in model_name:\n",
    "        embed_model = VoyageEmbedding(\n",
    "            model_name='voyage-02', voyage_api_key=voyage_api_key\n",
    "        )\n",
    "    elif 'instructor' in model_name:\n",
    "        embed_model = InstructorEmbedding(model_name=model_name)\n",
    "    else:\n",
    "        embed_model = HuggingFaceEmbedding(model_name=model_name, trust_remote_code=True)\n",
    "\n",
    "    # Create a service context\n",
    "    text_splitter = SentenceSplitter(chunk_size=chunk_size)\n",
    "    prompt_helper = PromptHelper(\n",
    "        context_window=context_window,\n",
    "        num_output=num_output,\n",
    "        chunk_overlap_ratio=0.1,\n",
    "        chunk_size_limit=None,\n",
    "    )\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        text_splitter=text_splitter,\n",
    "        prompt_helper=prompt_helper,\n",
    "    )\n",
    "    set_global_service_context(service_context)\n",
    "\n",
    "    # Now read the corpus json file.\n",
    "    # It should print the first record for debugging purposes\n",
    "    reader = JSONReader()\n",
    "    data = reader.load_data(corpus)\n",
    "    print('Corpus Data')\n",
    "    print('--------------------------')\n",
    "    print(data[0])\n",
    "    print('--------------------------')\n",
    "\n",
    "    # Now initialise the model. Download it if it is not cached.\n",
    "    # Finetune the embeddings for our corpus.\n",
    "    print('Initialising pipeline')\n",
    "    transformations = [text_splitter,CustomExtractor()]\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    nodes = pipeline.run(documents=data)\n",
    "    nodes_see = deepcopy(nodes)\n",
    "    print(\n",
    "        \"LLM sees:\\n\",\n",
    "        (nodes_see)[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    "    )\n",
    "    print('Finished Loading...')\n",
    "\n",
    "    index = VectorStoreIndex(nodes, show_progress=True)\n",
    "    print('Vector Store Created ...')\n",
    "\n",
    "    # Now we are finally ready to parse the queries.\n",
    "    with open(queries, 'r') as file:\n",
    "        query_data = json.load(file)\n",
    "\n",
    "    print('Query Data')\n",
    "    print('--------------------------')\n",
    "    print(query_data[0])\n",
    "    print('--------------------------')\n",
    "\n",
    "    if rerank:\n",
    "        print('Reranker enabled')\n",
    "        rerank_postprocessors = FlagEmbeddingReranker(model=rerank_model_name, top_n=topk)\n",
    "\n",
    "    # Run the retrieval. This will take a while.\n",
    "    retrieval_save_list = []\n",
    "    print(\"Running Retrieval ...\")\n",
    "    for data in tqdm(query_data):\n",
    "        query = data['query']\n",
    "        if rerank:\n",
    "            nodes_score = index.as_retriever(similarity_top_k=20).retrieve(query)\n",
    "            nodes_score = rerank_postprocessors.postprocess_nodes(\n",
    "                        nodes_score, query_bundle=QueryBundle(query_str=query)\n",
    "                    )\n",
    "        else:\n",
    "            nodes_score = index.as_retriever(similarity_top_k=topk).retrieve(query)\n",
    "        retrieval_list = []\n",
    "        for ns in nodes_score:\n",
    "            dic = {}\n",
    "            dic['text'] = ns.get_content(metadata_mode=MetadataMode.LLM)\n",
    "            dic['score'] = ns.get_score()\n",
    "            retrieval_list.append(dic)\n",
    "        save = {}\n",
    "        save['query'] = data['query']\n",
    "        save['answer'] = data['answer']\n",
    "        save['question_type'] = data['question_type']\n",
    "        save['retrieval_list'] = retrieval_list\n",
    "        save['gold_list'] = data['evidence_list']\n",
    "        retrieval_save_list.append(save)\n",
    "\n",
    "    print('Retieval complete. Saving Results')\n",
    "    with open(save_file, 'w') as json_file:\n",
    "        json.dump(retrieval_save_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbe3ff-7cc0-4e3f-87be-98c4f7217e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution loop.\n",
    "\n",
    "if STAGING:\n",
    "  corpus = \"data/sample-corpus.json\"\n",
    "  queries = \"data/sample-rag.json\"\n",
    "else:\n",
    "  corpus = \"data/corpus.json\"\n",
    "  queries = \"data/rag.json\"\n",
    "\n",
    "rank_model_name = \"BAAI/llm-embedder\"\n",
    "\n",
    "# Reranking requires both a rank and rerank model to be defined.\n",
    "rerank = True\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"\n",
    "output_name = \"output/bge-reranker-base.json\"\n",
    "\n",
    "# Run the main loop.\n",
    "# NOTE: jupyter already runs in an asyncio loop, so we have to use it to run our code.\n",
    "# Otherwise, we get an asyncio error that does not occur in the script version.\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(gen_stage_0(corpus, queries, rank_model_name, rerank, rerank_model_name, output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7f0e5-655a-4405-bf47-a27e90458503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13e233-b9b9-470c-ae3f-ea8c1d7ce8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
