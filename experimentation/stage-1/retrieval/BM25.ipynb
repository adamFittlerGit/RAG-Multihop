{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from llama_index import Document\n",
    "\n",
    "# This is the staging flag. Set to False if you want to run on the real\n",
    "# collection.\n",
    "# STAGING=False\n",
    "STAGING=True\n",
    "\n",
    "def save_list_to_json(lst, filename):\n",
    "  \"\"\" Save Files \"\"\"\n",
    "  with open(filename, 'w') as file:\n",
    "    json.dump(lst, file)\n",
    "\n",
    "def wr_dict(filename,dic):\n",
    "  \"\"\" Write Files \"\"\"\n",
    "  try:\n",
    "    if not os.path.isfile(filename):\n",
    "      data = []\n",
    "      data.append(dic)\n",
    "      with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    else:      \n",
    "      with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        data.append(dic)\n",
    "      with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "  except Exception as e:\n",
    "    print(\"Save Error:\", str(e))\n",
    "  return\n",
    "            \n",
    "def rm_file(file_path):\n",
    "  \"\"\" Delete Files \"\"\"\n",
    "  if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"File {file_path} removed successfully.\")\n",
    "\n",
    "def _depth_first_yield(json_data: Any, levels_back: int, collapse_length: \n",
    "                       Optional[int], path: List[str], ensure_ascii: bool = False,\n",
    "                      ) -> Generator[str, None, None]:\n",
    "  \"\"\" Do depth first yield of all of the leaf nodes of a JSON.\n",
    "      Combines keys in the JSON tree using spaces.\n",
    "      If levels_back is set to 0, prints all levels.\n",
    "      If collapse_length is not None and the json_data is <= that number\n",
    "      of characters, then we collapse it into one line.\n",
    "  \"\"\"\n",
    "  if isinstance(json_data, (dict, list)):\n",
    "    # only try to collapse if we're not at a leaf node\n",
    "    json_str = json.dumps(json_data, ensure_ascii=ensure_ascii)\n",
    "    if collapse_length is not None and len(json_str) <= collapse_length:\n",
    "      new_path = path[-levels_back:]\n",
    "      new_path.append(json_str)\n",
    "      yield \" \".join(new_path)\n",
    "      return\n",
    "    elif isinstance(json_data, dict):\n",
    "      for key, value in json_data.items():\n",
    "        new_path = path[:]\n",
    "        new_path.append(key)\n",
    "        yield from _depth_first_yield(value, levels_back, collapse_length, new_path)\n",
    "    elif isinstance(json_data, list):\n",
    "      for _, value in enumerate(json_data):\n",
    "        yield from _depth_first_yield(value, levels_back, collapse_length, path)\n",
    "    else:\n",
    "      new_path = path[-levels_back:]\n",
    "      new_path.append(str(json_data))\n",
    "      yield \" \".join(new_path)\n",
    "\n",
    "\n",
    "# The two classes are used to parse the json corpus and queries.\n",
    "class JSONReader():\n",
    "  \"\"\"JSON reader.\n",
    "     Reads JSON documents with options to help suss out relationships between nodes.\n",
    "  \"\"\"\n",
    "  def __init__(self, is_jsonl: Optional[bool] = False,) -> None:\n",
    "    \"\"\"Initialize with arguments.\"\"\"\n",
    "    super().__init__()\n",
    "    self.is_jsonl = is_jsonl\n",
    "\n",
    "  def load_data(self, input_file: str) -> List[Document]:\n",
    "    \"\"\"Load data from the input file.\"\"\"\n",
    "    documents = []\n",
    "    with open(input_file, 'r') as file:\n",
    "      load_data = json.load(file)\n",
    "    for data in load_data:\n",
    "      metadata = {\"title\": data['title'], \n",
    "                  \"published_at\": data['published_at'],\n",
    "                  \"source\":data['source']}\n",
    "      documents.append(Document(text=data['body'], metadata=metadata))\n",
    "    return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bm25(corpus, queries, output_name):\n",
    "    print('Remove save file if exists.')\n",
    "    rm_file(output_name)\n",
    "\n",
    "    # Read the corpus json file\n",
    "    reader = JSONReader()\n",
    "    data = reader.load_data(corpus)\n",
    "    \n",
    "    print('Corpus Data')\n",
    "    print('--------------------------')\n",
    "    print(data[0])\n",
    "    print('--------------------------')\n",
    "\n",
    "    corpus_texts = [doc.text for doc in data]\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in corpus_texts]\n",
    "\n",
    "    # Initialize BM25\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    print('BM25 Initialized ...')\n",
    "\n",
    "    # Parse the queries\n",
    "    with open(queries, 'r') as file:\n",
    "        query_data = json.load(file)\n",
    "\n",
    "    print('Query Data')\n",
    "    print('--------------------------')\n",
    "    print(query_data[0])\n",
    "    print('--------------------------')\n",
    "\n",
    "    retrieval_save_list = []\n",
    "    print(\"Running BM25 Retrieval ...\")\n",
    "\n",
    "    for data in tqdm(query_data):\n",
    "        query = data['query']\n",
    "        tokenized_query = query.split(\" \")\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top results\n",
    "        top_results = sorted(zip(scores, corpus_texts), reverse=True)[:10]\n",
    "\n",
    "        retrieval_list = []\n",
    "        for score, text in top_results:\n",
    "            dic = {}\n",
    "            dic['text'] = text\n",
    "            dic['score'] = score\n",
    "            retrieval_list.append(dic)\n",
    "\n",
    "        save = {}\n",
    "        save['query'] = data['query']\n",
    "        save['answer'] = data['answer']\n",
    "        save['question_type'] = data['question_type']\n",
    "        save['retrieval_list'] = retrieval_list\n",
    "        save['gold_list'] = data['evidence_list']\n",
    "        retrieval_save_list.append(save)\n",
    "\n",
    "    print('Retrieval complete. Saving Results')\n",
    "    with open(output_name, 'w') as json_file:\n",
    "        json.dump(retrieval_save_list, json_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if STAGING:\n",
    "        corpus = \"data/sample-corpus.json\"\n",
    "        queries = \"data/sample-rag.json\"\n",
    "    else:\n",
    "        corpus = \"data/corpus.json\"\n",
    "        queries = \"data/rag.json\"\n",
    "        \n",
    "    output_name = \"output/bm25-retrieval.json\"\n",
    "\n",
    "    gen_bm25(corpus, queries, output_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
