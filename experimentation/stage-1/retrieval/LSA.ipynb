{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from llama_index import Document\n",
    "\n",
    "STAGING = True\n",
    "\n",
    "# Helper functions for file handling (same as before)\n",
    "def save_list_to_json(lst, filename):\n",
    "    \"\"\" Save Files \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(lst, file)\n",
    "\n",
    "def rm_file(file_path):\n",
    "    \"\"\" Delete Files \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} removed successfully.\")\n",
    "\n",
    "class JSONReader:\n",
    "    \"\"\"JSON reader.\"\"\"\n",
    "    def __init__(self, is_jsonl: Optional[bool] = False,) -> None:\n",
    "        \"\"\"Initialize with arguments.\"\"\"\n",
    "        super().__init__()\n",
    "        self.is_jsonl = is_jsonl\n",
    "\n",
    "    def load_data(self, input_file: str) -> List[Document]:\n",
    "        \"\"\"Load data from the input file.\"\"\"\n",
    "        documents = []\n",
    "        with open(input_file, 'r') as file:\n",
    "            load_data = json.load(file)\n",
    "        for data in load_data:\n",
    "            metadata = {\"title\": data['title'], \n",
    "                        \"published_at\": data['published_at'],\n",
    "                        \"source\":data['source']}\n",
    "            documents.append(Document(text=data['body'], metadata=metadata))\n",
    "        return documents\n",
    "\n",
    "def gen_lsa(corpus, queries, output_name):\n",
    "    print('Remove save file if exists.')\n",
    "    rm_file(output_name)\n",
    "\n",
    "    # Read the corpus json file\n",
    "    reader = JSONReader()\n",
    "    data = reader.load_data(corpus)\n",
    "    \n",
    "    print('Corpus Data')\n",
    "    print('--------------------------')\n",
    "    print(data[0])\n",
    "    print('--------------------------')\n",
    "\n",
    "    corpus_texts = [doc.text for doc in data]\n",
    "\n",
    "    # Create TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus_texts)\n",
    "\n",
    "    # Apply LSA using TruncatedSVD\n",
    "    lsa = TruncatedSVD(n_components=100)\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "    \n",
    "    print('LSA Initialized ...')\n",
    "\n",
    "    # Parse the queries\n",
    "    with open(queries, 'r') as file:\n",
    "        query_data = json.load(file)\n",
    "\n",
    "    print('Query Data')\n",
    "    print('--------------------------')\n",
    "    print(query_data[0])\n",
    "    print('--------------------------')\n",
    "\n",
    "    retrieval_save_list = []\n",
    "    print(\"Running LSA Retrieval ...\")\n",
    "\n",
    "    for data in tqdm(query_data):\n",
    "        query = data['query']\n",
    "        query_vector = vectorizer.transform([query])\n",
    "        query_lsa = lsa.transform(query_vector)\n",
    "\n",
    "        # Calculate cosine similarity between query and documents\n",
    "        scores = (X_lsa @ query_lsa.T).flatten()\n",
    "        \n",
    "        # Get top results\n",
    "        top_results = sorted(zip(scores, corpus_texts), reverse=True)[:10]\n",
    "\n",
    "        retrieval_list = []\n",
    "        for score, text in top_results:\n",
    "            dic = {}\n",
    "            dic['text'] = text\n",
    "            dic['score'] = score\n",
    "            retrieval_list.append(dic)\n",
    "\n",
    "        save = {}\n",
    "        save['query'] = data['query']\n",
    "        save['answer'] = data['answer']\n",
    "        save['question_type'] = data['question_type']\n",
    "        save['retrieval_list'] = retrieval_list\n",
    "        save['gold_list'] = data['evidence_list']\n",
    "        retrieval_save_list.append(save)\n",
    "\n",
    "    print('Retrieval complete. Saving Results')\n",
    "    with open(output_name, 'w') as json_file:\n",
    "        json.dump(retrieval_save_list, json_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if STAGING:\n",
    "        corpus = \"data/sample-corpus.json\"\n",
    "        queries = \"data/sample-rag.json\"\n",
    "    else:\n",
    "        corpus = \"data/corpus.json\"\n",
    "        queries = \"data/rag.json\"\n",
    "        \n",
    "    output_name = \"output/lsa-retrieval.json\"\n",
    "\n",
    "    gen_lsa(corpus, queries, output_name)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
