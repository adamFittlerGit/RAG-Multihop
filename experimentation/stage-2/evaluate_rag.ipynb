{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa7e42-2d31-4fa2-851f-4b5fa3f83f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter",
    "\n",
    "# This is the staging flag. Set to False if you want to run on the real\n",
    "# collection.\n",
    "# STAGING=False\n",
    "STAGING=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9783f-9a44-43a0-97aa-3a2e3b2cae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the correct answer\n",
    "def get_gold(query_data, query):\n",
    "  for q in query_data:\n",
    "    if q['query'] == query:\n",
    "      return q['answer']\n",
    "  return ''\n",
    "\n",
    "# Function to check if there is an intersection of words between two strings\n",
    "def has_intersection(a, b):\n",
    "  a_words = set(a.split())\n",
    "  b_words = set(b.split())\n",
    "  return len(a_words.intersection(b_words)) > 0\n",
    "\n",
    "# Function to extract the answer\n",
    "def extract_answer(input_string):\n",
    "  match = re.search(r'The answer to the question is \"(.*?)\"', input_string)\n",
    "  return match.group(1) if match else input_string\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def comp_metrics(pred_list, gold_list):\n",
    "  tp = sum(1 for pred, gold in zip(pred_list, gold_list)\n",
    "           if has_intersection(pred.lower(), gold.lower()))\n",
    "  fp = sum(1 for pred, gold in zip(pred_list, gold_list)\n",
    "           if not has_intersection(pred.lower(), gold.lower()))\n",
    "  fn = len(gold_list) - tp\n",
    "  precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "  recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "  f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "  return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cde9c7-c591-46f9-a499-ba75ede8a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(predictions, gold_labels):\n",
    "  # Read files\n",
    "  with open(predictions, 'r') as fh:\n",
    "    data = fh.read()\n",
    "    doc_data = json.loads(data)\n",
    "\n",
    "  #with open('dataset/MultiHopRAG.json', 'r') as file:\n",
    "  with open(gold_labels, 'r') as fh:\n",
    "    data = fh.read()\n",
    "    query_data = json.loads(data)\n",
    "\n",
    "  # Initialize dictionary to save lists of predictions and gold standards\n",
    "  # for each question_type\n",
    "  type_data = {}\n",
    "  overall_pred_list = []\n",
    "  overall_gold_list = []\n",
    "\n",
    "  #print(doc_data)\n",
    "  # Main loop, iterate through document data\n",
    "  for d in tqdm(doc_data):\n",
    "    model_answer = d['model_answer']\n",
    "    if 'The answer' in model_answer:\n",
    "      model_answer = extract_answer(model_answer)\n",
    "    gold = get_gold(query_data,d['query'])\n",
    "    if gold:\n",
    "      question_type = d['question_type']\n",
    "      if question_type not in type_data:\n",
    "        type_data[question_type] = {'pred_list': [], 'gold_list': []}\n",
    "      type_data[question_type]['pred_list'].append(model_answer)\n",
    "      type_data[question_type]['gold_list'].append(gold)\n",
    "      overall_pred_list.append(model_answer)\n",
    "      overall_gold_list.append(gold)\n",
    "\n",
    "  # Output evaluation data for each question_type\n",
    "  for question_type, data in type_data.items():\n",
    "    precision, recall, f1 = comp_metrics(data['pred_list'], data['gold_list'])\n",
    "    print(f\"Question Type: {question_type}\")\n",
    "    print(f\" Precision: {precision:.2f}\")\n",
    "    print(f\" Recall: {recall:.2f}\")\n",
    "    print(f\" F1 Score: {f1:.2f}\")\n",
    "    print()\n",
    "\n",
    "  # Calculate overall evaluation metrics\n",
    "  overall_precision, overall_recall, overall_f1 = comp_metrics(overall_pred_list,\n",
    "                                                               overall_gold_list)\n",
    "  print(f\"Overall Metrics:\")\n",
    "  print(f\" Precision: {overall_precision:.2f}\")\n",
    "  print(f\" Recall: {overall_recall:.2f}\")\n",
    "  print(f\" F1 Score: {overall_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65dd1f-31a3-4c71-9c90-7b8f334f5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop\n",
    "prediction_file = 'output/llama2.json'\n",
    "if STAGING:\n",
    "    gold_labels = 'data/sample-rag.json'\n",
    "else:\n",
    "    gold_labels = 'data/rag.json'\n",
    "\n",
    "run_evaluation(prediction_file, gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f224b-a8c3-4d67-bc62-5c0d9c71491a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
